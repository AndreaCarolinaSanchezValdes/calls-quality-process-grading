{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT LIBRARIES AND SET THE NOTEBOOK\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 30\n",
    "pd.set_option('max_columns', 500), pd.set_option('float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ DATA BASE, FORMAT and DEFINE PORTFOLIO TYPE AND AGREEMENTS\n",
    "bd11=pd.read_excel(\"acuerdo_plazo_castigo.xlsx\")\n",
    "bd11.columns=bd11.columns.str.lower()\n",
    "bd11['Tipo de cartera']=\"Campaña castigada\"\n",
    "bd11['Tipo de Acuerdos']=\"Hubo acuerdo\"\n",
    "bd12=pd.read_excel(\"no_hubo_acuerdo.xlsx\")\n",
    "bd12.columns=bd12.columns.str.lower()\n",
    "bd12['Tipo de cartera']=\"Campaña castigada\"\n",
    "bd12['Tipo de Acuerdos']=\"No hubo acuerdo\"\n",
    "bd13=pd.read_excel(\"acuerdo_de_pago_vigente.xlsx\")\n",
    "bd13.columns=bd13.columns.str.lower()\n",
    "bd13['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd13['Tipo de Acuerdos']=\"Hubo acuerdo\"\n",
    "bd14=pd.read_excel(\"no_hubo_acuerdo_vigente.xlsx\")\n",
    "bd14.columns=bd14.columns.str.lower()\n",
    "bd14['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd14['Tipo de Acuerdos']=\"No hubo acuerdo\"\n",
    "bd15=pd.read_excel(\"ampliacion_pago_con_castigo.xlsx\")\n",
    "bd15.columns=bd15.columns.str.lower()\n",
    "bd15['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd15['Tipo de Acuerdos']=\"Todas\"\n",
    "bd16=pd.read_excel(\"ampliacion_plazo_dia.xlsx\")\n",
    "bd16.columns=bd16.columns.str.lower()\n",
    "bd16['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd16['Tipo de Acuerdos']=\"Todas\"\n",
    "bd17=pd.read_excel(\"congelamiento_capital_credicompras.xlsx\")\n",
    "bd17.columns=bd17.columns.str.lower()\n",
    "bd17['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd17['Tipo de Acuerdos']=\"Todas\"\n",
    "bd18=pd.read_excel(\"congelamiento_total_credicompras.xlsx\")\n",
    "bd18.columns=bd18.columns.str.lower()\n",
    "bd18['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd18['Tipo de Acuerdos']=\"Todas\"\n",
    "bd19=pd.read_excel(\"descuento_al_dia.xlsx\")\n",
    "bd19.columns=bd19.columns.str.lower()\n",
    "bd19['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd19['Tipo de Acuerdos']=\"Todas\"\n",
    "bd20=pd.read_excel(\"descuento_con_bloqueo.xlsx\")\n",
    "bd20.columns=bd20.columns.str.lower()\n",
    "bd20['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd20['Tipo de Acuerdos']=\"Todas\"\n",
    "bd21=pd.read_excel(\"modificacion_cuota.xlsx\")\n",
    "bd21.columns=bd21.columns.str.lower()\n",
    "bd21['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd21['Tipo de Acuerdos']=\"Todas\"\n",
    "bd22=pd.read_excel(\"modificacion_cuota_sin_deterioro.xlsx\")\n",
    "bd22.columns=bd22.columns.str.lower()\n",
    "bd22['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd22['Tipo de Acuerdos']=\"Todas\"\n",
    "bd23=pd.read_excel(\"reestructuracion.xlsx\")\n",
    "bd23.columns=bd23.columns.str.lower()\n",
    "bd23['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd23['Tipo de Acuerdos']=\"Todas\"\n",
    "bd24=pd.read_excel(\"Congelamiento Privada.xlsx\")\n",
    "bd24.columns=bd24.columns.str.lower()\n",
    "bd24['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd24['Tipo de Acuerdos']=\"Todas\"\n",
    "bd1=pd.concat([bd11,bd12,bd13,bd14,bd15,bd16,bd17,bd18,bd19,bd20,bd21,bd22,bd23,bd24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary type configuration dataframe\n",
    "bd_aux=bd1.copy()\n",
    "bd_aux=bd_aux[['id','Tipo de cartera','Tipo de Acuerdos']]\n",
    "bd_aux['id']=bd_aux['id'].str.replace('\\\\(|\\\\)','')\n",
    "bd_aux['id']=bd_aux['id'].str.replace('[^0-9]',\"\")\n",
    "bd_aux['id']=bd_aux['id'].str.strip()\n",
    "bd_aux=bd_aux.drop_duplicates()\n",
    "bd_aux=bd_aux.reset_index(drop=True)\n",
    "bd_aux.columns=['CallID','Tipo de cartera','Tipo de Acuerdos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPLICATES THE DATABASE OBTAINED ABOVE TO GENERATE ANOTHER POSSIBLE COMBINATION OF AGREEMENTS AND PORTFOLIO\n",
    "bd2=bd1.copy()\n",
    "bd2['Tipo de cartera']=\"Todas\"\n",
    "bd2['Tipo de Acuerdos']=\"Todas\"\n",
    "bd1=pd.concat([bd1,bd2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd11['Tipo de cartera']=\"Campaña castigada\"\n",
    "bd11['Tipo de Acuerdos']=\"Todas\"\n",
    "bd12['Tipo de cartera']=\"Campaña castigada\"\n",
    "bd12['Tipo de Acuerdos']=\"Todas\"\n",
    "bd1=pd.concat([bd1,bd11,bd12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd11['Tipo de cartera']=\"Todas\"\n",
    "bd11['Tipo de Acuerdos']=\"Hubo acuerdo\"\n",
    "bd12['Tipo de cartera']=\"Todas\"\n",
    "bd12['Tipo de Acuerdos']=\"No hubo acuerdo\"\n",
    "bd1=pd.concat([bd1,bd11,bd12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd13['Tipo de cartera']=\"Todas\"\n",
    "bd13['Tipo de Acuerdos']=\"Hubo acuerdo\"\n",
    "bd14['Tipo de cartera']=\"Todas\"\n",
    "bd14['Tipo de Acuerdos']=\"No hubo acuerdo\"\n",
    "bd1=pd.concat([bd1,bd13,bd14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd13['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd13['Tipo de Acuerdos']=\"Todas\"\n",
    "bd14['Tipo de cartera']=\"Campaña vigente\"\n",
    "bd14['Tipo de Acuerdos']=\"Todas\"\n",
    "bd1=pd.concat([bd1,bd13,bd14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(bd11,bd12,bd13,bd14,bd15,bd16,bd17,bd18,bd19,bd20,bd21,bd22,bd23,bd24)\n",
    "bd1=bd1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ AUDIO AND DICTIONARY BASE DATABASES\n",
    "#dictionary\n",
    "bd3=pd.read_excel(\"DICCIONARIO.xlsx\",\n",
    "       sheet_name='CARTERA VIGENTE CASTIGADA',skiprows=1,usecols=['Bloque', 'Atributo', 'Subtopico', 'Palabra - frase',\n",
    "       'Tipo de Atributo', 'Tipo de cartera','Tipo de Acuerdos', 'Tipo de llamada', 'Tipo de producto'])\n",
    "#metadata audios\n",
    "carpeta= r'C:\\Users\\Archivos_Metadata\\Archivos_Metadata'\n",
    "os.chdir(carpeta)\n",
    "excel = glob.glob('*.xlsx')\n",
    "list_data = []\n",
    "for filename in excel:\n",
    "    try:\n",
    "        data = pd.read_excel(filename)\n",
    "        list_data.append(data)\n",
    "        bd4=pd.concat(list_data,ignore_index=True)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        continue\n",
    "bd4.columns=bd4.columns.str.lower()\n",
    "#data consultants\n",
    "bd5=pd.read_excel(\"Nombre asesor.xlsx\",usecols=['Nombre Asesor','Usuario'])\n",
    "bd5.columns=bd5.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION OF CLEANING, special characters and punctuation marks and mails\n",
    "def limpieza_basica(body):\n",
    "    a,b = 'áéíóúüñÁÉÍÓÚÜàèìòùÃ?','aeiouunAEIOUUaeiounn'\n",
    "    trans = str.maketrans(a,b)\n",
    "    body=body.translate(trans)\n",
    "    body = body.lower()\n",
    "    body = re.sub(\"\\\\d+\",'', body)\n",
    "    body = re.sub('[%s]' %re.escape(string.punctuation),' ', body) \n",
    "    body= re.compile(r'\\bLiza\\b').sub('', body) \n",
    "    body=re.sub(r'([\\w\\d\\.]+)@[\\w\\d\\.]+' , \"\", body)\n",
    "    return body\n",
    "limpiar = lambda x: limpieza_basica(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning function simpler for columns other than transcription\n",
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "        \n",
    "    s = s.replace('\\d+',' ').replace(' +', ' ').replace('[^\\w\\s]+',' ')\n",
    "    s = s.replace('\\\\[|\\\\]','')\n",
    "    s = s.replace('\\n',' ')\n",
    "    s = s.replace('xx+', '')\n",
    "    s = s.replace('\\s\\s+', ' ')\n",
    "    s = s.strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply cleanup Y variable is defined TextoLimpio that will have all the changes applied to the conversation\n",
    "bd1['TextoLimpio']=bd1['transcript'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "bd1['TextoLimpio']=bd1['transcript'].apply(limpiar)\n",
    "bd1['TextoLimpio']=bd1['TextoLimpio'].str.replace('\\d+',' ').str.replace(' +', ' ').str.replace('\\n+',' ').str.replace('áéíóúñ', 'aeioun').str.replace(u\"á\", \"a\").str.replace(u\"é\", \"e\").str.replace(u\"í\", \"i\").str.replace(u\"ó\", \"o\").str.replace(u\"ú\", \"u\").str.replace(u\"ñ\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove \".wav\" extension from the call ids that are the caller ID of the calling customer\n",
    "bd1['id']=bd1['id'].str.replace('[^0-9]',\"\")\n",
    "bd1['id']=bd1['id'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define columns and column names in the transcript dataframe\n",
    "df = bd1[['id', 'canal', 'transcript','TextoLimpio','Tipo de cartera','Tipo de Acuerdos']]\n",
    "df.columns = ['CallID','Canal','Conversación','TextoLimpio','Tipo de cartera','Tipo de Acuerdos']\n",
    "del(bd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n"
     ]
    }
   ],
   "source": [
    "#delete empty conversations\n",
    "df.dropna(subset=['Conversación'], inplace=True) \n",
    "print(len(df['CallID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dataframe on which to start working is generated\n",
    "BD_Speech=df.copy()\n",
    "BD_Speech[\"CallID\"] = BD_Speech[\"CallID\"].astype(object)\n",
    "del(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning of columns portfolio, agreements and key\n",
    "BD_Speech['Tipo de cartera']=BD_Speech['Tipo de cartera'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_Speech['Tipo de cartera']=BD_Speech['Tipo de cartera'].apply(lambda x: normalize(x))\n",
    "BD_Speech['Tipo de Acuerdos']=BD_Speech['Tipo de Acuerdos'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_Speech['Tipo de Acuerdos']=BD_Speech['Tipo de Acuerdos'].apply(lambda x: normalize(x))\n",
    "#crossing key with dictionary\n",
    "BD_Speech['key']=BD_Speech['Tipo de cartera']+BD_Speech['Tipo de Acuerdos']\n",
    "BD_Speech['key'] = BD_Speech['key'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_Speech['key'] = BD_Speech['key'].apply(lambda x: normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_Speech['CallID'].isnull().sum() \n",
    "lista  = BD_Speech['CallID'].unique()\n",
    "concat = pd.DataFrame(columns = ['CallID', 'Asesor','Cliente']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete concatenation of the conversations between the consultant and the client, everything said by the consultant and everything said by the client is concatenated. \n",
    "# and it is filtered with the key so as not to repeat the conversation several times.\n",
    "BD_Speech2=BD_Speech[['CallID','Canal','TextoLimpio']]\n",
    "BD_Speech2=BD_Speech2.drop_duplicates()\n",
    "for id in lista:\n",
    "    bd_concat=BD_Speech2[BD_Speech2['CallID']==id]\n",
    "    callid = bd_concat[bd_concat['CallID']==id]['CallID'].unique()[0] #guarda el nombre del callid\n",
    "    asesor = ' '.join(str(i) for i in bd_concat[(bd_concat['CallID'] == id) & (bd_concat['Canal'] == 1)]['TextoLimpio'])\n",
    "    cliente = ' '.join(str(i) for i in bd_concat[(bd_concat['CallID'] == id) & (bd_concat['Canal'] == 2)]['TextoLimpio'])\n",
    "    concat=concat.append({'CallID': callid ,'Asesor' : asesor,'Cliente' : cliente} , ignore_index=True)\n",
    "\n",
    "concat['Asesor'] = (\" \" + concat['Asesor'] + \" \")\n",
    "concat['Asesor'] = concat['Asesor'].apply(lambda x: re.sub(' +', ' ', x))\n",
    "concat['Cliente'] = (\" \" + concat['Cliente'] + \" \")\n",
    "concat['Cliente'] = concat['Cliente'].apply(lambda x: re.sub(' +', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concat.columns = [\"CallID\", \"TS_Agente\",\"TS_Cliente\"]\n",
    "concat[\"CallID\"]=concat[\"CallID\"].astype(object)\n",
    "concat['TS_Agente']=concat['TS_Agente'].replace(\" \",None)\n",
    "concat['TS_Cliente']=concat['TS_Cliente'].replace(\" \",None)\n",
    "concat=concat[concat['TS_Agente'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BD_model_frs=pd.merge(BD_Speech,concat,on='CallID',how='left')\n",
    "del(concat)\n",
    "del(BD_Speech,BD_Speech2)\n",
    "del(bd_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs=BD_model_frs[['CallID', 'Tipo de cartera',\n",
    "       'Tipo de Acuerdos', 'key', 'TS_Agente', 'TS_Cliente']]\n",
    "BD_model_frs=BD_model_frs.drop_duplicates()\n",
    "BD_model_frs=BD_model_frs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more cleaning of the consultant's conversation in the desc variable\n",
    "BD_model_frs[\"desc\"] = BD_model_frs[\"TS_Agente\"].str.replace('\\d+',' ').str.replace(' +', ' ').str.replace('[^\\w\\s]+',' ')\n",
    "BD_model_frs[\"desc\"] = BD_model_frs[\"desc\"].str.replace('\\n',' ')#elimina saltos de linea\n",
    "BD_model_frs[\"desc\"] = BD_model_frs[\"desc\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model_frs[\"desc\"] = BD_model_frs[\"desc\"].str.lower()\n",
    "BD_model_frs[\"desc\"] = BD_model_frs[\"desc\"].str.replace('xx+', '')\n",
    "BD_model_frs[\"desc\"] = BD_model_frs[\"desc\"].str.replace('\\s\\s+', ' ')\n",
    "BD_model_frs[\"desc\"] = BD_model_frs[\"desc\"].str.strip()\n",
    "BD_model_frs[\"desc\"] = BD_model_frs[\"desc\"].astype(str)\n",
    "BD_model_frs=BD_model_frs[BD_model_frs[\"desc\"]!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more cleaning of the consultant's conversation in the desc2 variable\n",
    "BD_model_frs[\"desc2\"] = BD_model_frs[\"TS_Cliente\"].str.replace('\\d+',' ').str.replace(' +', ' ').str.replace('[^\\w\\s]+',' ')\n",
    "BD_model_frs[\"desc2\"] = BD_model_frs[\"desc2\"].str.replace('\\n',' ')#elimina saltos de linea\n",
    "BD_model_frs[\"desc2\"] = BD_model_frs[\"desc2\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model_frs[\"desc2\"] = BD_model_frs[\"desc2\"].apply(limpiar)\n",
    "BD_model_frs[\"desc2\"] = BD_model_frs[\"desc2\"].str.lower()\n",
    "BD_model_frs[\"desc2\"] = BD_model_frs[\"desc2\"].str.replace('xx+', '')\n",
    "BD_model_frs[\"desc2\"] = BD_model_frs[\"desc2\"].str.replace('\\s\\s+', ' ')\n",
    "BD_model_frs[\"desc2\"] = BD_model_frs[\"desc2\"].str.strip()\n",
    "BD_model_frs[\"desc2\"] = BD_model_frs[\"desc2\"].astype(str)\n",
    "BD_model_frs=BD_model_frs[BD_model_frs[\"desc2\"]!=\"\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GREETING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GREETING SENTENCES - asks for customer name, required to remove customer name from the conversation\n",
    "bd31=bd3[(bd3['Subtopico']=='Pregunta nombre del cliente')]\n",
    "bd31[\"Palabra - frase\"]=bd31[\"Palabra - frase\"].str.lower()\n",
    "#first look for a pattern in the dictionary and separate the phrases by that pattern.\n",
    "bd31[\"Palabra - frase\"] = bd31[\"Palabra - frase\"].str.replace('xx+','xxx')\n",
    "patron = \"|\".join([\"xxx\"])\n",
    "bd31[\"encontro\"]=bd31[\"Palabra - frase\"].str.findall(patron)\n",
    "bd31['encontro']=bd31['encontro'].astype(str).str.replace('\\\\[|\\\\]','')\n",
    "bd31=bd31[~bd31['encontro'].isin([\"\"])]\n",
    "bd31[\"Palabra - frase\"]=bd31[\"Palabra - frase\"].apply(limpiar)\n",
    "bd31=pd.DataFrame(bd31[\"Palabra - frase\"].str.split(\"xxx\",1,expand= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the first and second part of the sentence separated, a new search pattern is generated for the consultant's conversations.\n",
    "# start sentences and end sentences (before and after the client's name)\n",
    "bd31.columns=['frase_ini','frase_fin']\n",
    "bd31['frase_ini']=bd31['frase_ini'].apply(limpiar)\n",
    "bd31['frase_fin']=bd31['frase_fin'].apply(limpiar)\n",
    "bd31['frase_ini'] = bd31['frase_ini'].str.replace('\\s\\s+',\" \")\n",
    "bd31['frase_fin'] = bd31['frase_fin'].str.replace('\\s\\s+',\" \")\n",
    "bd31['frase_ini'] = bd31['frase_ini'].str.strip()\n",
    "bd31['frase_fin'] = bd31['frase_fin'].str.strip()\n",
    "bd31['frase_fin'] = bd31['frase_fin'].replace('si',None)\n",
    "bd31.dropna(subset=['frase_fin'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For initial phrase\n",
    "bd31['n_ini'] = bd31['frase_ini'].apply(lambda x: len(x))\n",
    "bd31.sort_values(['frase_ini', 'n_ini'], ascending=False, inplace=True)\n",
    "frases_ini = bd31['frase_ini'].unique()\n",
    "#  For end phrase\n",
    "bd31['n_fin'] = bd31['frase_fin'].apply(lambda x: len(x))\n",
    "bd31.sort_values(['frase_fin', 'n_fin'], ascending=False, inplace=True)\n",
    "frases_fin = bd31['frase_fin'].unique()\n",
    "\n",
    "patron_ini = '(^|\\s)' + '(' + '|'.join(frases_ini) + ')' \n",
    "patron_fin = '(' + '$|' + '|'.join(frases_fin) + ')' + '(\\s|$)'\n",
    "#the final pattern is defined, we try to find the second part of the pattern in the phrase of the greeting and that this is a name\n",
    "patron = patron_ini + '\\s?(.*?)\\s?' + patron_fin\n",
    "BD_model_frs['hallazgo'] = BD_model_frs['desc'].apply(lambda x:re.findall(patron,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The second sentence found by findall in the first part of the conversation is assumed to be the name of the client.\n",
    "bd32=BD_model_frs[['CallID','hallazgo']]\n",
    "bd32=bd32.drop_duplicates('CallID')\n",
    "bd32=bd32[bd32['hallazgo'].notnull()]\n",
    "bd_hall = pd.DataFrame(columns = ['CallID', 'hallazgo','hallazgo2'])\n",
    "for id in lista:\n",
    "    try:\n",
    "        bd33=bd32[bd32['CallID']==id]\n",
    "        callid=bd33[bd33['CallID']==id]['CallID'].unique()[0]\n",
    "        hallazgo=list(bd33['hallazgo'])\n",
    "        #extracts the name found by the findall according to the patterns\n",
    "        hallazgo2=hallazgo[0][0][2]\n",
    "        bd_hall=bd_hall.append({'CallID': callid ,'hallazgo' : hallazgo,'hallazgo2' : hallazgo2} , ignore_index=True)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only those whose length is less than or equal to 4 words are extracted for security reasons, \n",
    "# in case what was found is not a name or if there was a transcription error and there were similar phrases among those searched by the pattern.\n",
    "from collections import Counter\n",
    "bd_hall['cantidad_palabras2']=bd_hall[\"hallazgo2\"].apply(lambda x:len(x))\n",
    "bd_hall[\"hallazgo2\"]=np.where(bd_hall['cantidad_palabras2']==2,'',bd_hall[\"hallazgo2\"]) #if the patterns found are of two letters, they are not taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_hall['cantidad_palabras']=bd_hall[\"hallazgo2\"].apply(lambda x: Counter(x.split(' ')))\n",
    "bd_hall['cantidad_palabras']=bd_hall[\"cantidad_palabras\"].apply(lambda x:len(x))\n",
    "bd_hall[\"hallazgo2\"]=np.where(bd_hall['cantidad_palabras']>4,None,bd_hall[\"hallazgo2\"])\n",
    "bd_hall=bd_hall[~bd_hall[\"hallazgo2\"].isin(['se registra','me confirma','claro que si claro','si entiendo si','ah bueno','en la linea','a bueno si',\n",
    "'me alegra mucho','me confirma su nombre','a saber','tengo el gusto hablar','tengo el gusto hablar','ver salio pena la','muchisimas gracias'])]\n",
    "bd_hall=bd_hall[bd_hall[\"hallazgo2\"].notnull()]\n",
    "bd_hall=bd_hall[~bd_hall[\"hallazgo2\"].isin([\"\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dijeron angel flores|simon el hudson|anibal santana anibal|maria buitrago|diego alejandro agudelo mejia|simon coronel|juan guillermo arbelaez urrea|manuel jurado palomino|florencio rivero pico|luis fernando valencia gracias|maria isabel bonilla|milagroso se arrieta lo|marta me alegra mucho|john tovar|gonzalo urrutia|daniel rivas|enrique salazar caicedo|jose leonardo barreto barrera|ricardo rodriguez solo|pedro indicame|guzman muy buenas tardes|carlos|lina osorio|marco antonio martinez clavijo|ignacio enrique porras manjarrez|miguel angel viafara|juana francisca zuleta herrera|carlos duarte|raul londono|marta usted|maria elena|carmenza bocanegra|juan carlos rojas|carolina caballero|johan jose bracho moreno|hector gonzalez|henry garcia|jenni blandon|este santos|luz adriana seria seria|andres felipe garcia cruz|haidi|orlando claro|martha cecilia ballesteros barrera|gloria esperanza ovalle jimenez|mariela marulanda como esa|rodolfo peluffo zuniga|yolanda garcia|denise los anos garavito|andrea carolina ramirez oh|juliana raquel perez|luigi manrique roja|luz estela pineros|elizabeth ceballos rojas gracias|diana marcela burgos|alvaro william oliveros cordoba|rosalba esther carbonell blanco|maria marulanda|yolanda garcia un motor|maricela echavarria|silvia|doris martinez|elvia delgado|elsa marina guerrero garcia|jesus grajales|luz estela bedoya|carmen daisy ramos|guillermo guillermo ever sanchez|tuya catalan hablo|pedro zuloaga buenas tardes|maria nelly castillo estupinan|raul farfan|lilia ines cadavid|juan gabriel duran cuesta|rosa delgado|rubiela figueroa figueredo|rafael eduardo ramirez perez|marta|gladys de jesus perez|monica alar|rafael ramirez|maria cardenas|oscar marino gomez vazquez|jorge isaac parrado reyes|maria victoria sosa hablo|rosa gutierrez alo alo|raul linares|jose david quiroga quiroga charo|fernando antonio contreras|dagoberto vargas|nidia chica|john mauro patino valencia|alba liliana velasquez henao|jose rojas tio jorge|sara quiroz|german cierra|polidoro arroyo cierto libro|luis barriga|julio|constantino ramirez|alejandro luis alejandro mojica|henry benavides|maria chamarra|luis lo|ivan mendez|jose dice buenas tardes|juan jose gonzalez|pedro|jose rojas al|pedro agustin|oscar|oscar|a michael serna|andrea soledad soledad noli|luis gomez|johan diaz|jorge garcia|muy buenas tardes|roberto hernandez|alex mano a|jorge caicedo|alicia rodriguez|hermeregildo rin|cesar camacho|jessica begazo|antonio romero antonio|gustavo janes buenas noches|joselito rodriguez ando|jaime jimenez|wanxin|victor leon|jose andrade|jose velazquez san jose|flavio carbone|serafin castro|francisco angulo|luis fernando duque el|german baquero|william sarmiento|german ricardo ramirez|franklin salinas|franklin salinas|serafin castro el|jorge eliecer borges|juan aguirre alo|si julio perez la|rafael salazar entiendo si|meme de mi momento|eliana zambrano|carlos gonzalez|hector godoy|alvaro sierra|jose forero|pedro quintero|florentino pinzon|carlos mayorga la|william pastran|fernando paez|jorge rin|jose luis jimenez|olegario rivas es|luis castro|alvaro sanchez|hermogenes ramirez|juan gonzalez|nicolas roa como disculpame|carlos garcia|francisco altahona altahona altahona|pedro vargas|carlos julio dias|jose domingo suave|juan gomez|jose molina|pedro manta|pablo ayala|de lili estevez la|jorge reyes|santiago umba buenas tardes|jose solis ando|si buenas noches|luis chavez|carmen uzcategui solo|evelyn nunez|eva perez|jose|jorge guaman|carlos rin|luis rodriguez|diego mira|diego cadavid|leonardo no pues|freddy ramirez|cesar ortiz|german mendez|alberto osorio|carlos nada el|rene mesa de comunicando|nestor enrique|humberto pineda|edwin rodriguez|el verso|hector jaime franco|jose si|ivan dario trujillo|ricardo es|climaco aguirre|valentin cardenas es|luis ovalle|fabian enrique arias|rodrigo gaytan|hyperspeed|jose el|jose solano|alfonso marin el|jose rodriguez|nicolas giovanni guillen|luis escobar|ruben gimenez habla|luis rico me olvides|gerardo rojas|jairo castillo y|darlington bermudez alo|juan norena|freddy poveda habla|walter gonzalez|efectivamente bueno|rafael aldana|simon|abraham delgado tener habra|antonio sainz|zerpa|ricardo alanis si|jose|antonio zambrano mira antonio|hernando barrios muy bien|jose hablar|alberto turizo|jorge montoya|alfredo permitame estoy verificando|luis hola si san luis san luis|juan baena correcto listo|pedro narvaez|milton|gerardo flores|margarita muneton|leslie gutierrez|juan rodriguez|maria quinones|jenny vasquez|ruth marianela ramirez el|elena davila|jose esteban|milagros corredor|orlando montes|libertad romero|libertad|andrea castellanos|jose velazquez|omar lozano|guillermo lopez|eduardo barcenas|luis pineda|santiago chimba co|juan carlos zambrano|cristian galeano|jose f|luz acevedo|nelson suarez|carlos leon|hector gutierrez|janet alvarado|janet alvarado|carolina martinez|rafael ballen|adriana parra buen dia|gustavo lopez|antonio castaneda|marin roman y|carlos me da|juan manuel zapata|luis alvarez|adriana rios|oscar narvaez|edgar coronado|pedro rodriguez|jorge guazumal|edgar caceres si|jorge rojas hola|alejandro betancourt|jorge barrera|dario guerra alo|marcela garcia|johnny machado te|jessy benito villa|jorge canon ah bueno|como le puedo ayudar|avisa y tu que|rene tarango|rene perez|alberto ramos alberto|jesus flores|carolina beltran si|alvaro cordoba|rosalba castillo|omar dias|flor pena|sandra alfonso|sandra alfonso|armando acosta|luz villamil|xiomara delgado|vio quemar verenzuela pablo|edmundo timaran el|johan mosquera hola|claudia beltran si|armando garcia|clara hinestroza|permitame|edilma castellanos|adolfo rosado ando|leonardo correa|jorge k no|carlos rojas|fray mendez|oscar restrepo|claudia cuadros|la confirmacion a tania|maria hincapie|omar morales buen dia|jose|guillermo urzola|pedro|alvaro ceballos|johana johana|margarita dias|margarita dias|lina valderrama que pena|ya que en martinez|maria alcantara mira maria|maria|amable la|sor maria sanchez|edwin bolanos|alba gonzalez|wilmer enchila|hector rodriguez|maria reina buenas tardes|tito cortes|juan play|margarita gordillo|paola gomez|liliana son gusta muy|mercy suarez|andrea victoria insua si|reynaldo rios|elizabeth marin|marta pero habla|buenas noches|alo regalame|oscar riveros|juan machado|juan machado|raul arciniegas|alfredo|y usted la confirmaria|emiro barrera|carlos humberto marin|alba mendieta|de ermita momento|andres que anda alo|wilmer linares|emma te yo si|luis macias|jorge benavides|marcelino ordonez marcelino|luis castillo|benedicto ya de peso|samir hernandez|angel pineda buenas noches|angel pineda buenas noches|antonio rojas en antonio|jorge guerrero|luis alo buenas tardes|hernando me confirmas|carlos lara|omaira ninos|john lopez|elkin restrepo|simon rodriguez buen dia|javier ramirez|rafael torres|santos mendez|jose fuentes|victor barrera|hilder carreno|hector saul|felipe kajiado|luis eduardo rengifo|uber henao|victor perez hola|nelson ramirez|jairo alexander|jose alexander reyes si|antonio restrepo|pedro cardona a pedro|jose avila|juan roberto angulo|francisco leal|saul fajardo|nelson acevedo|guillermo cortes|porfirio|marco montana|net cordoba|angelopilis quinto|fernando torres|luis sopo|juan|orlando bonilla|diomedes moreno|jorge dias|alvaro salazar audio|hugo vega|luis molina|jose me los debia|rafael castillo|rosalba barrera'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The list of names found by the employer is generated, these will be removed from the final transcript of the consultant's conversation.\n",
    "frases_hall= bd_hall[\"hallazgo2\"]\n",
    "frases_hall='|'.join(frases_hall)\n",
    "frases_hall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRODUCT TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd4=bd4[['cedula', 'nro obligación', 'fecha gestion', 'nombre de producto',\n",
    "       'dias de mora', 'desc ultimo codigo de gestion prejuridico','grabador','nota', 'nombre causal']]\n",
    "bd4['cedula']=bd4['cedula'].astype(str)\n",
    "bd4['cedula']=bd4['cedula'].str.strip()\n",
    "bd4['cedula']=bd4['cedula'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data of audios when the file names are not the IDs\n",
    "info_audios=pd.read_excel(\"Informacion_Audios.xlsx\",usecols=['Carpeta',  'id'])\n",
    "info_audios.columns=['cedula', 'CallID']\n",
    "info_audios['cedula']=info_audios['cedula'].astype(str)\n",
    "info_audios['cedula']=info_audios['cedula'].str.strip()\n",
    "info_audios['cedula']=info_audios['cedula'].astype(object)\n",
    "info_audios['CallID']=info_audios['CallID'].astype(str)\n",
    "info_audios['CallID']=info_audios['CallID'].str.strip()\n",
    "info_audios[\"CallID\"]=info_audios[\"CallID\"].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd4=pd.merge(bd4,info_audios,on='cedula',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs=pd.merge(BD_model_frs,bd4,on='CallID',how='left')\n",
    "del(bd4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_productos=['MASTERCARD','CREDICOMPRAS']\n",
    "productos='|'.join(lista_productos)\n",
    "BD_model_frs['Tipo de producto']=BD_model_frs['nombre de producto'].str.findall(productos)\n",
    "BD_model_frs[\"Tipo de producto\"]=BD_model_frs[\"Tipo de producto\"].astype(str).str.replace('\\\\[|\\\\]','')\n",
    "BD_model_frs['Tipo de producto']=BD_model_frs['Tipo de producto'].replace('nan','Todas')\n",
    "BD_model_frs['Tipo de producto']=BD_model_frs['Tipo de producto'].replace('','Todas')\n",
    "BD_model_frs['Tipo de producto']=BD_model_frs['Tipo de producto'].replace(\"'MASTERCARD'\",'Mastercard')\n",
    "BD_model_frs['Tipo de producto']=BD_model_frs['Tipo de producto'].replace(\"'CREDICOMPRAS'\",'Credicompras')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRAUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fraude|estafa|robar|robaron|pqr|robada|robo|le vayan a responder por algun valor|fiscalia|robaron la tarjeta|tarjeta no tiene ninguna seguridad|inseguridad'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dictionary of phrases related to fraud is defined.\n",
    "frases_fraude= [\"fraude\",'estafa','robar','robaron','pqr','robada','robo','le vayan a responder por algun valor','fiscalia','robaron la tarjeta',\n",
    "'tarjeta no tiene ninguna seguridad','inseguridad']\n",
    "frases_fraude='|'.join(frases_fraude)\n",
    "frases_fraude"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary cleaning\n",
    "DIC_FIN0=bd3.copy()\n",
    "DIC_FIN0['Atributo']=DIC_FIN0['Atributo'].apply(lambda x: normalize(x))\n",
    "DIC_FIN0['Bloque']=DIC_FIN0['Bloque'].apply(lambda x: normalize(x))\n",
    "DIC_FIN0['Subtopico']=DIC_FIN0['Subtopico'].apply(lambda x: normalize(x))\n",
    "DIC_FIN0['Tipo de Atributo']=DIC_FIN0['Tipo de Atributo'].apply(lambda x: normalize(x))\n",
    "DIC_FIN0['Tipo de cartera']=DIC_FIN0['Tipo de cartera'].apply(lambda x: normalize(x))\n",
    "DIC_FIN0['Tipo de Acuerdos']=DIC_FIN0['Tipo de Acuerdos'].apply(lambda x: normalize(x))\n",
    "\n",
    "DIC_FIN0['Palabra - frase']=DIC_FIN0['Palabra - frase'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "DIC_FIN0['Palabra - frase']=DIC_FIN0['Palabra - frase'].apply(limpiar)\n",
    "DIC_FIN0['Palabra - frase']=DIC_FIN0['Palabra - frase'].str.lower()\n",
    "DIC_FIN0[\"Palabra - frase\"] = DIC_FIN0[\"Palabra - frase\"].apply(lambda x: normalize(x))\n",
    "\n",
    "DIC_FIN0[\"Palabra - frase\"] = DIC_FIN0[\"Palabra - frase\"].str.strip()\n",
    "DIC_FIN0 = DIC_FIN0.drop_duplicates(\"Palabra - frase\")\n",
    "del(bd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a space on either side to single-word phrases so that it does not identify words that contain a piece of the dictionary definition.DIC_FIN0['cantidad_palabras']=DIC_FIN0[\"Palabra - frase\"].apply(lambda x: Counter(x.split(' ')))\n",
    "DIC_FIN0['cantidad_palabras']=DIC_FIN0[\"cantidad_palabras\"].apply(lambda x:len(x))\n",
    "DIC_FIN0[\"Palabra - frase\"]=np.where(DIC_FIN0['cantidad_palabras']==1,(\" \"+DIC_FIN0[\"Palabra - frase\"]+\" \"),DIC_FIN0[\"Palabra - frase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize product type\n",
    "DIC_FIN0['Tipo de producto']=DIC_FIN0['Tipo de producto'].replace(\"N/A MasterCard\",'Mastercard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create key in the dictionary\n",
    "DIC_FIN0['key']=DIC_FIN0['Tipo de cartera']+DIC_FIN0['Tipo de Acuerdos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the dictionary from the longest to the shortest sentences to maximize the use of findall\n",
    "DIC_FIN0=DIC_FIN0.sort_values(by=[\"cantidad_palabras\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "DIC_FIN0['Atributo']=DIC_FIN0['Atributo'].apply(lambda x: normalize(x))\n",
    "DIC_FIN0['Bloque']=DIC_FIN0['Bloque'].apply(lambda x: normalize(x))\n",
    "DIC_FIN0['Subtopico']=DIC_FIN0['Subtopico'].apply(lambda x: normalize(x))\n",
    "DIC_FIN0['Tipo de Atributo']=DIC_FIN0['Tipo de Atributo'].apply(lambda x: normalize(x))\n",
    "DIC_FIN0['Tipo de cartera']=DIC_FIN0['Tipo de cartera'].apply(lambda x: normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary direct attributes\n",
    "DIC_FIN_ = DIC_FIN0[(DIC_FIN0[\"Tipo de Atributo\"]==\"Directo\")]\n",
    "#creates list of direct attributes\n",
    "Atributos=list(DIC_FIN_[\"Atributo\"].unique())\n",
    "DIC_FIN2=DIC_FIN_[DIC_FIN_.Atributo.isin(Atributos)]\n",
    "DIC_FIN2[\"Palabra - frase\"]=DIC_FIN2[\"Palabra - frase\"].str.replace(\"  \",\" \")\n",
    "frases2 = DIC_FIN2[\"Palabra - frase\"]\n",
    "frases2 = \"|\".join(frases2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDENTIFICATION OF INDIRECT ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminates the names of identified customers and applies more clean up\n",
    "BD_model_frs[\"desc\"]=BD_model_frs[\"desc\"].str.replace(frases_hall,'xxx')\n",
    "BD_model_frs[\"desc\"]=BD_model_frs[\"desc\"].apply(lambda x: normalize(x))\n",
    "BD_model_frs[\"desc\"]=BD_model_frs[\"desc\"].str.replace(\"xxx\",\" \")\n",
    "BD_model_frs[\"desc\"]=BD_model_frs[\"desc\"].str.replace('\\n+',' ')\n",
    "BD_model_frs[\"desc\"]=BD_model_frs[\"desc\"].str.replace('\\s\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify if there were any phrases referring to fraud and flag the conversation\n",
    "BD_model_frs[\"fraude_cliente\"]=BD_model_frs[\"desc2\"].str.findall(frases_fraude)\n",
    "BD_model_frs[\"fraude_asesor\"]=BD_model_frs[\"desc\"].str.findall(frases_fraude)\n",
    "BD_model_frs[\"fraude_cliente\"] = BD_model_frs[\"fraude_cliente\"].astype(str).str.replace('\\\\[|\\\\]','')\n",
    "BD_model_frs[\"fraude_asesor\"] = BD_model_frs[\"fraude_asesor\"].astype(str).str.replace('\\\\[|\\\\]','')\n",
    "BD_model_frs[\"fraude_cliente2\"] = np.where(BD_model_frs[\"fraude_cliente\"]=='','NO','SI')\n",
    "BD_model_frs[\"fraude_asesor2\"] = np.where(BD_model_frs[\"fraude_asesor\"]=='','NO','SI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the basis on which the indirect attributes will be worked with\n",
    "BD_model_frs1 = BD_model_frs[['CallID', 'Tipo de cartera',\n",
    "       'Tipo de Acuerdos', 'TS_Agente','TS_Cliente', 'desc', 'desc2','nro obligación', 'fecha gestion', 'nombre de producto', 'dias de mora',\n",
    "       'desc ultimo codigo de gestion prejuridico','grabador',\n",
    "       'nombre causal','key','fraude_asesor2', 'fraude_asesor', 'fraude_cliente2', 'fraude_cliente']]\n",
    "BD_model_frs1 = BD_model_frs1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_id_trans=BD_model_frs1[['CallID','desc']]\n",
    "BD_id_trans=BD_id_trans.drop_duplicates()\n",
    "BD_id_trans=BD_id_trans.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for dictionary phrases whose attribute type is indirect, the search is performed by subtopic.\n",
    "bloques=list(DIC_FIN0[\"Subtopico\"].unique())\n",
    "BD_model_frs111=pd.DataFrame()\n",
    "for i in bloques:\n",
    "    DIC_FIN = DIC_FIN0[(DIC_FIN0[\"Tipo de Atributo\"]==\"Indirecto\")]\n",
    "    DIC_FIN = DIC_FIN[(DIC_FIN[\"Subtopico\"]==i)]\n",
    "    \n",
    "    Atributos=list(DIC_FIN[\"Atributo\"].unique())\n",
    "    DIC_FIN1=DIC_FIN[DIC_FIN.Atributo.isin(Atributos)]\n",
    "    frases= DIC_FIN1[\"Palabra - frase\"]\n",
    "    frases='|'.join(frases)\n",
    "    BD_model_frs11=BD_id_trans.copy()\n",
    "    BD_model_frs11[\"Palabra - frase\"] = BD_model_frs11[\"desc\"].str.findall(frases)\n",
    "    BD_model_frs111=pd.concat([BD_model_frs111,BD_model_frs11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(BD_model_frs11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs11=BD_model_frs111.explode('Palabra - frase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs11[\"Palabra - frase\"] = BD_model_frs11[\"Palabra - frase\"].str.replace('[^\\w\\s]+','') #reemplaza todo aquello diferente de letras\n",
    "BD_model_frs11[\"Palabra - frase\"] = BD_model_frs11[\"Palabra - frase\"].str.strip() #eliminando espacios antes y despues de la frase\n",
    "BD_model_frs11 = BD_model_frs11.drop_duplicates()\n",
    "del(BD_model_frs111,BD_id_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs1=pd.merge(BD_model_frs1,BD_model_frs11,on=['CallID','desc'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization of columns for merge\n",
    "DIC_FIN0[\"Palabra - frase\"] = DIC_FIN0[\"Palabra - frase\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "DIC_FIN0[\"Palabra - frase\"] = DIC_FIN0[\"Palabra - frase\"].apply(lambda x: normalize(x))\n",
    "DIC_FIN0[\"key\"] = DIC_FIN0[\"key\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "DIC_FIN0[\"key\"] = DIC_FIN0[\"key\"].apply(lambda x: normalize(x))\n",
    "BD_model_frs1=BD_model_frs1[BD_model_frs1[\"Palabra - frase\"].notnull()]\n",
    "BD_model_frs1=BD_model_frs1[~BD_model_frs1[\"Palabra - frase\"].isin([''])]\n",
    "BD_model_frs1[\"Palabra - frase\"] = BD_model_frs1[\"Palabra - frase\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model_frs1[\"Palabra - frase\"] = BD_model_frs1[\"Palabra - frase\"].apply(lambda x: normalize(x))\n",
    "BD_model_frs1[\"key\"] = BD_model_frs1[\"key\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model_frs1[\"key\"] = BD_model_frs1[\"key\"].apply(lambda x: normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs1 = pd.merge(BD_model_frs1,DIC_FIN0,on= [\"Palabra - frase\",'key'],how = \"left\")\n",
    "BD_model_frs1 = BD_model_frs1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs1['Bloque'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs1['Atributo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null block is filtered because it means that the word or phrase found does not apply to that type of portfolio and agreement.\n",
    "BD_model_frs1=BD_model_frs1[BD_model_frs1['Bloque'].notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDENTIFICATION OF DIRECT ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs2 = BD_model_frs[['CallID', 'Tipo de cartera',\n",
    "       'Tipo de Acuerdos', 'TS_Agente','TS_Cliente', 'desc', 'desc2','nro obligación', 'fecha gestion', 'nombre de producto', 'dias de mora',\n",
    "       'desc ultimo codigo de gestion prejuridico','grabador',\n",
    "       'nombre causal','key','fraude_asesor2', 'fraude_asesor', 'fraude_cliente2', 'fraude_cliente']]\n",
    "BD_model_frs2 = BD_model_frs2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs2[\"Palabra - frase\"] = BD_model_frs2[\"desc\"].str.findall(frases2)\n",
    "BD_model_frs2=BD_model_frs2.explode('Palabra - frase')\n",
    "BD_model_frs2[\"Palabra - frase\"] = BD_model_frs2[\"Palabra - frase\"].str.replace('[^\\w\\s]+','')\n",
    "BD_model_frs2[\"Palabra - frase\"] = BD_model_frs2[\"Palabra - frase\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs2=BD_model_frs2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC_FIN2[\"Palabra - frase\"] = DIC_FIN2[\"Palabra - frase\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "DIC_FIN2[\"Palabra - frase\"] = DIC_FIN2[\"Palabra - frase\"].apply(lambda x: normalize(x))\n",
    "DIC_FIN2[\"key\"] = DIC_FIN2[\"key\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "DIC_FIN2[\"key\"] = DIC_FIN2[\"key\"].apply(lambda x: normalize(x))\n",
    "BD_model_frs2=BD_model_frs2[BD_model_frs2[\"Palabra - frase\"].notnull()]\n",
    "BD_model_frs2=BD_model_frs2[~BD_model_frs2[\"Palabra - frase\"].isin([''])]\n",
    "BD_model_frs2[\"Palabra - frase\"] = BD_model_frs2[\"Palabra - frase\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model_frs2[\"Palabra - frase\"] = BD_model_frs2[\"Palabra - frase\"].apply(lambda x: normalize(x))\n",
    "BD_model_frs2[\"key\"] = BD_model_frs2[\"key\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model_frs2[\"key\"] = BD_model_frs2[\"key\"].apply(lambda x: normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs2 = BD_model_frs2.merge(DIC_FIN2, how = \"left\", on= [\"Palabra - frase\",'key'])\n",
    "BD_model_frs2 = BD_model_frs2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs2['Bloque'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs2['Atributo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model_frs2=BD_model_frs2[BD_model_frs2['Bloque'].notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates dataframe to define the qualification\n",
    "BD_model=pd.concat([BD_model_frs1,BD_model_frs2])\n",
    "del(BD_model['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_model=BD_model[['CallID', 'Tipo de cartera_x', 'Tipo de Acuerdos_x',\n",
    "       'TS_Agente','TS_Cliente',\n",
    "       'desc','desc2' , 'nro obligación', 'fecha gestion',\n",
    "       'nombre de producto', 'dias de mora',\n",
    "       'desc ultimo codigo de gestion prejuridico', 'nombre causal',\n",
    "       'Palabra - frase', 'Bloque', 'Atributo', 'Subtopico',\n",
    "       'Tipo de Atributo','Tipo de llamada', 'Tipo de producto','grabador','fraude_cliente', 'fraude_asesor','fraude_cliente2', 'fraude_asesor2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BD_model.columns=['CallID','Tipo de cartera', 'Tipo de Acuerdos',\n",
    "       'TS_Agente', 'TS_Cliente','desc','desc2', 'nro obligación', 'fecha gestion',\n",
    "       'nombre de producto', 'dias de mora',\n",
    "       'desc ultimo codigo de gestion prejuridico', 'nombre causal',\n",
    "       'Palabra - frase', 'Bloque', 'Atributo', 'Subtopico',\n",
    "       'Tipo de Atributo','Tipo de llamada', 'Tipo de producto','grabador','fraude_cliente', 'fraude_asesor','fraude_cliente2', 'fraude_asesor2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BD_model['Atributo']=BD_model['Atributo'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model['Bloque']=BD_model['Bloque'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model['Subtopico']=BD_model['Subtopico'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model['Tipo de Atributo']=BD_model['Tipo de Atributo'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model['Tipo de cartera']=BD_model['Tipo de cartera'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_model['Tipo de Acuerdos']=BD_model['Tipo de Acuerdos'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BD_model1=BD_model[['CallID', 'Palabra - frase', 'Bloque', 'Atributo', 'Subtopico',\n",
    "       'Tipo de Atributo', 'Tipo de cartera', 'Tipo de Acuerdos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading of the dictionary summary support guide\n",
    "from sqlalchemy import Float\n",
    "bd_guia=pd.read_excel(\"GUIA.xlsx\",sheet_name=['GUIA USAR'],\n",
    "usecols=['Bloque', 'Atributo', 'Subtopico','Tipo de Atributo','Tipo de cartera', 'Tipo de Acuerdos', 'Tipo de error','Pesos'],dtype={'Pesos': Float})\n",
    "bd_guia=bd_guia.get('GUIA USAR')\n",
    "bd_guia.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning of the guide\n",
    "bd_guia['Atributo']=bd_guia['Atributo'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "bd_guia['Bloque']=bd_guia['Bloque'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "bd_guia['Subtopico']=bd_guia['Subtopico'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "bd_guia['Tipo de Atributo']=bd_guia['Tipo de Atributo'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "bd_guia['Tipo de cartera']=bd_guia['Tipo de cartera'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "bd_guia['Tipo de Acuerdos']=bd_guia['Tipo de Acuerdos'].apply(lambda x: normalize(x)).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "bd_guia=bd_guia.drop_duplicates()\n",
    "bd_guia=bd_guia[bd_guia['Tipo de Atributo'].notnull()]\n",
    "bd_guia=bd_guia.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of phrases found by suptopico in each call\n",
    "BD_resumen=BD_model1.groupby(['Bloque', 'Atributo', 'Subtopico','Tipo de Atributo','Tipo de cartera', \n",
    "                                'Tipo de Acuerdos'])['CallID'].value_counts().reset_index(name='Cantidad de frases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_resumen2=pd.merge(BD_resumen,bd_guia,on=['Bloque', 'Atributo', 'Subtopico','Tipo de Atributo','Tipo de cartera', \n",
    "                                'Tipo de Acuerdos'],how='left')\n",
    "BD_resumen2.sort_values(by=['CallID','Bloque','Atributo','Subtopico'])\n",
    "BD_resumen2=BD_resumen2[BD_resumen2['CallID'].notnull()]\n",
    "BD_resumen2['key2']=BD_resumen2['Bloque']+BD_resumen2['Atributo']+BD_resumen2['Subtopico']+BD_resumen2['Tipo de Atributo']+BD_resumen2['Tipo de error']+BD_resumen2['Tipo de cartera']+BD_resumen2['Tipo de Acuerdos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the validation guide\n",
    "bd_guia_val=bd_guia.copy()\n",
    "bd_guia_val['Cantidad de frases']=0\n",
    "bd_guia_val['key2']=bd_guia_val['Bloque']+bd_guia_val['Atributo']+bd_guia_val['Subtopico']+bd_guia_val['Tipo de Atributo']+bd_guia_val['Tipo de error']+bd_guia_val['Tipo de cartera']+bd_guia_val['Tipo de Acuerdos']\n",
    "bd_guia_val=bd_guia_val[bd_guia_val['key2'].notnull()]\n",
    "bd_guia_val=bd_guia_val.drop_duplicates()\n",
    "bd_guia_val=bd_guia_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_guia_val['key2']=bd_guia_val['key2'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "bd_guia_val['key2']=bd_guia_val['key2'].apply(lambda x: normalize(x))\n",
    "BD_resumen2.dropna(subset=['key2'], inplace=True) \n",
    "BD_resumen2['key2']=BD_resumen2['key2'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "BD_resumen2['key2']=BD_resumen2['key2'].apply(lambda x: normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added to the summary base the subtopics of the call not found\n",
    "BD_resumen3=pd.DataFrame()\n",
    "for i in lista:\n",
    "    bd_llam=BD_resumen2[BD_resumen2['CallID']==i]\n",
    "    bd_falta=bd_guia_val[~bd_guia_val.key2.isin(bd_llam['key2'].unique())]\n",
    "    bd_falta['CallID']=i\n",
    "    bd_falta=bd_falta.drop_duplicates()\n",
    "    bd_llam=pd.concat([bd_llam,bd_falta])\n",
    "    BD_resumen3=pd.concat([BD_resumen3,bd_llam])\n",
    "BD_resumen3=BD_resumen3.sort_values(by=['CallID','Bloque','Atributo','Subtopico'])\n",
    "BD_resumen3=BD_resumen3.drop_duplicates()\n",
    "del(BD_resumen3['key2'])\n",
    "BD_resumen3=BD_resumen3.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PENALTY CONDITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines penalties according to whether the attribute is indirect or direct.\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['Cantidad de frases']==0) & (BD_resumen3['Tipo de Atributo']=='Indirecto'),'SI',None)\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['Cantidad de frases']!=0) & (BD_resumen3['Tipo de Atributo']=='Indirecto'),'NO',BD_resumen3['penaliza'])\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['Cantidad de frases']==0) & (BD_resumen3['Tipo de Atributo']=='Directo'),'NO',BD_resumen3['penaliza'])\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['Cantidad de frases']!=0) & (BD_resumen3['Tipo de Atributo']=='Directo'),'SI',BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valid not to penalize calls that are of the opposite type of agreement to the one to be penalized.\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de Acuerdos'].isin(['No hubo acuerdo'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Tipo de Acuerdos'].isin(['Hubo acuerdo'])),'NO',BD_resumen3['penaliza'])\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de Acuerdos'].isin(['Hubo acuerdo'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Tipo de Acuerdos'].isin(['No hubo acuerdo'])),'NO',BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valid not to penalize calls that are of the opposite type of agreement to the one to be penalized.\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de cartera'].isin(['Campaña castigada'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Tipo de cartera'].isin(['Campana vigente'])),'NO',BD_resumen3['penaliza'])\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de cartera'].isin(['Campaña vigente'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Tipo de cartera'].isin(['Campana castigada'])),'NO',BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#does not penalize subtopics exclusive to a portfolio type\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de cartera'].isin(['Campaña vigente'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Subtopico'].isin(['Solicita linea adicional del cliente'])),'NO',\n",
    "                                    BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the negotiation block is only evaluated for NO agreement calls.\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de Acuerdos'].isin(['Hubo acuerdo'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Bloque']=='Negociacion'),'NO',BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the subtopic 'Report total balance' is only evaluated for penalized portfolio calls.\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de cartera'].isin(['Campaña vigente'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Subtopico'].isin(['Informa el saldo total'])),'NO',BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the subtopic 'Report minimum balance' is only evaluated for calls from current portfolio\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de cartera'].isin(['Campaña castigada'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Subtopico'].isin(['Informa el pago minimo'])),'NO',BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtopics that only apply in the case of non-agreement calls\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de Acuerdos'].isin(['No hubo acuerdo'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Subtopico'].isin(['Confirma valor del acuerdo',\n",
    "                                    'Confirma fecha del acuerdo','Indica medios de pago'])),'NO',\n",
    "                                    BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtopics that only apply in the case of portfolio calls in force there was agreement\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de Acuerdos'].isin(['No hubo acuerdo'])]\n",
    "bd_aux2=bd_aux2[bd_aux2['Tipo de cartera'].isin(['Campaña vigente'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "BD_resumen3['penaliza']=np.where((BD_resumen3['CallID'].isin(bd_aux2['CallID'].unique()))&(BD_resumen3['Subtopico'].isin(['Confirma correo electronico',\n",
    "                                    'Indica ANS para solicitar paz y salvo','Indica donde debe solicitar paz y salvo','Solicita cumplimiento del pago'])),'NO',\n",
    "                                    BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ofrece alternativas se hace validación #offers alternatives double validation is done if the call is for the current campaign since this subtopic has phrases that apply to all or for the current campaign.doble si la llamada es de Campaña vigente ya que este subtopico tiene frases que aplican para todas o para Campaña vigente\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de cartera'].isin(['Campaña vigente'])]\n",
    "bd_aux2=bd_aux2[bd_aux2['Tipo de Acuerdos'].isin(['No hubo acuerdo'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "for i in list(bd_aux2['CallID']):\n",
    "    bd_aux3=BD_resumen3[(BD_resumen3['CallID']==i)&(BD_resumen3['Subtopico']=='Ofrece alternativas de pago')]\n",
    "    pen=list(bd_aux3['penaliza'].unique())\n",
    "    if \"NO\" in pen:\n",
    "        BD_resumen3['penaliza']=np.where((BD_resumen3['CallID']==i)&(BD_resumen3['Subtopico']=='Ofrece alternativas de pago'),'NO',BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate ownership is double validated if the call is from a penalized campaign since this subtopic has phrases that apply to all or penalized campaigns.\n",
    "#is left even if currently valid title is not being penalized.\n",
    "bd_aux2=bd_aux[bd_aux['Tipo de cartera'].isin(['Campaña castigada'])]\n",
    "bd_aux2=bd_aux2[['CallID']]\n",
    "for i in list(bd_aux2['CallID']):\n",
    "    bd_aux3=BD_resumen3[(BD_resumen3['CallID']==i)&(BD_resumen3['Subtopico']=='Valida la titularidad')]\n",
    "    pen=list(bd_aux3['penaliza'].unique())\n",
    "    if \"NO\" in pen:\n",
    "        BD_resumen3['penaliza']=np.where((BD_resumen3['CallID']==i)&(BD_resumen3['Subtopico']=='Valida la titularidad'),'NO',BD_resumen3['penaliza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a count of the number of penalties\n",
    "BD_calificacion=BD_resumen3.groupby(['CallID','Bloque', 'Atributo', 'Subtopico','Tipo de Atributo','Tipo de cartera', \n",
    "                                'Tipo de Acuerdos','Tipo de error','Pesos'])['penaliza'].value_counts().reset_index(name='cantidad_penalizaciones')\n",
    "BD_calificacion=BD_calificacion.sort_values(by=['CallID','Tipo de error'])\n",
    "BD_calificacion['cantidad_penalizaciones']=np.where(BD_calificacion['penaliza'].isin(['NO']),0,BD_calificacion['cantidad_penalizaciones'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_aux=BD_resumen3[['CallID','Bloque', 'Atributo', 'Subtopico','Tipo de Atributo','Tipo de cartera', \n",
    "                                'Tipo de Acuerdos','Cantidad de frases']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_calificacion=pd.merge(BD_calificacion,BD_aux,on=['CallID','Bloque', 'Atributo', 'Subtopico','Tipo de Atributo','Tipo de cartera', \n",
    "                                'Tipo de Acuerdos'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_calificacion_ecn=pd.DataFrame()\n",
    "for i in lista:\n",
    "    BD_calificacion2=BD_calificacion[(BD_calificacion['Tipo de error']=='ECN')&(BD_calificacion['CallID']==i)]\n",
    "    x=\"SI\" in list(BD_calificacion2['penaliza'].unique())\n",
    "    if x==True:\n",
    "        BD_calificacion3=pd.DataFrame([[i,0]],index=[0],columns =['CallID','ECN'])\n",
    "    elif x==False:\n",
    "        BD_calificacion3=pd.DataFrame([[i,1]],index=[0],columns =['CallID','ECN'])\n",
    "        \n",
    "    BD_calificacion_ecn=pd.concat([BD_calificacion_ecn,BD_calificacion3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_calificacion_ecu=pd.DataFrame()\n",
    "for i in lista:\n",
    "    BD_calificacion2=BD_calificacion[(BD_calificacion['Tipo de error']=='ECU')&(BD_calificacion['CallID']==i)]\n",
    "    x=\"SI\" in list(BD_calificacion2['penaliza'].unique())\n",
    "    if x==True:\n",
    "        BD_calificacion3=pd.DataFrame([[i,0]],index=[0],columns =['CallID','ECU'])\n",
    "    elif x==False:\n",
    "        BD_calificacion3=pd.DataFrame([[i,1]],index=[0],columns =['CallID','ECU'])\n",
    "        \n",
    "    BD_calificacion_ecu=pd.concat([BD_calificacion_ecu,BD_calificacion3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_calificacion_enc=pd.DataFrame()\n",
    "for i in lista:\n",
    "    BD_calificacion2=BD_calificacion[(BD_calificacion['Tipo de error']=='ENC')&(BD_calificacion['CallID']==i)]\n",
    "    x=\"SI\" in list(BD_calificacion2['penaliza'].unique())\n",
    "    if x==True:\n",
    "        BD_calificacion2['peso_penalizado']=BD_calificacion2['Pesos']*BD_calificacion2['cantidad_penalizaciones']\n",
    "        valor_penalizado=1-sum(BD_calificacion2['peso_penalizado'])\n",
    "        BD_calificacion3=pd.DataFrame([[i,valor_penalizado]],index=[0],columns =['CallID','ENC'])\n",
    "    elif x==False:\n",
    "        BD_calificacion3=pd.DataFrame([[i,1]],index=[0],columns =['CallID','ENC'])\n",
    "        \n",
    "    BD_calificacion_enc=pd.concat([BD_calificacion_enc,BD_calificacion3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVERALL RATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD_calificacion_grl=pd.merge(BD_calificacion_ecu,BD_calificacion_enc,on=['CallID'],how='left')\n",
    "BD_calificacion_grl=pd.merge(BD_calificacion_grl,BD_calificacion_ecn,on=['CallID'],how='left')\n",
    "BD_calificacion_grl=BD_calificacion_grl.drop_duplicates()\n",
    "BD_calificacion_grl=pd.merge(BD_calificacion_grl,bd_aux,on=['CallID'],how='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93626ebdf6b37739e4ac5700c0dba13041517f6c4b50580fe5c850e4df425864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
